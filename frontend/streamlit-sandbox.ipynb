{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def describe_image(image_path):\n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Validate API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OpenAI API key not found in environment variables\")\n",
    "            \n",
    "        # Validate image path\n",
    "        if not image_path:\n",
    "            raise ValueError(\"Image path cannot be empty\")\n",
    "\n",
    "        # Convert the image to base64\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Using the correct model for vision\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error describing image: {str(e)}\")\n",
    "    \n",
    "def extract_text_from_img(image_path):\n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Validate API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OpenAI API key not found in environment variables\")\n",
    "            \n",
    "        # Validate image path\n",
    "        if not image_path:\n",
    "            raise ValueError(\"Image path cannot be empty\")\n",
    "\n",
    "        # Convert the image to base64\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Using the correct model for vision\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Extract text from this image only. Do not include any other information.\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error describing image: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_image(image_path='img.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_from_img(image_path='img.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.llms import Ollama\n",
    "from PIL import Image\n",
    "import PyPDF2\n",
    "import base64\n",
    "import io\n",
    "\n",
    "def init_ollama():\n",
    "    return Ollama(model=\"llama3.2:latest\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def describe_image(image_file):\n",
    "    try:\n",
    "        # Open and process the image\n",
    "        image = Image.open(image_file)\n",
    "        \n",
    "        # Convert PIL Image to base64 string\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "        \n",
    "        # Create a more detailed prompt\n",
    "        prompt = (\"Provide a detailed description of this image, including: \"\n",
    "                 \"main subjects, colors, composition, lighting, setting, \"\n",
    "                 \"and any notable details or activities shown.\")\n",
    "    \n",
    "        try:\n",
    "            # Initialize Ollama with vision model\n",
    "            llm = init_ollama()\n",
    "            \n",
    "            # Pass the base64 encoded image string\n",
    "            description = llm(prompt, images=[img_str])\n",
    "            return description\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error generating description: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading image: {str(e)}\")\n",
    "\n",
    "# Create the Streamlit interface\n",
    "st.title('Sandbox for Marketing Materials Compliance Analysis')\n",
    "\n",
    "# Initialize chat history in session state if it doesn't exist\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.file_uploader(\"Upload a PDF or Image file\", type=['pdf', 'png', 'jpg', 'jpeg'])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    try:\n",
    "        # Process based on file type\n",
    "        file_type = uploaded_file.type\n",
    "        if 'pdf' in file_type:\n",
    "            text_content = extract_text_from_pdf(uploaded_file)\n",
    "            st.success(\"PDF processed successfully!\")\n",
    "            \n",
    "            # Create prompt for summarization\n",
    "            prompt = f\"Please summarize the following text:\\n\\n{text_content}\"\n",
    "            \n",
    "            # Generate summary using Ollama\n",
    "            llm = init_ollama()\n",
    "            response = llm(prompt)\n",
    "            \n",
    "        elif 'image' in file_type:\n",
    "            # Display the uploaded image\n",
    "            st.image(uploaded_file, caption=\"Uploaded Image\")\n",
    "            \n",
    "            # Get image description\n",
    "            response = describe_image(uploaded_file)\n",
    "            st.success(\"Image processed successfully!\")\n",
    "        \n",
    "        # Add the response to chat history\n",
    "        st.session_state.messages.append({\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": f\"Analysis of uploaded file:\\n\\n{response}\"\n",
    "        })\n",
    "\n",
    "        st.session_state.uploaded_file = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Chat input for follow-up questions\n",
    "if prompt := st.chat_input(\"Ask questions about the uploaded content\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Display user message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Generate response\n",
    "    try:\n",
    "        llm = init_ollama()\n",
    "        response = llm(prompt)\n",
    "\n",
    "        # Display assistant response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(response)\n",
    "        \n",
    "        # Add assistant response to chat history\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error communicating with Ollama: {str(e)}\")\n",
    "\n",
    "# Add a button to clear chat history\n",
    "if st.button(\"Clear Chat History\"):\n",
    "    st.session_state.messages = []\n",
    "    st.experimental_rerun()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                      ID              SIZE      MODIFIED     \n",
      "llava:7b                  8dd30f6b0cb1    4.7 GB    27 hours ago    \n",
      "llama3.2-vision:latest    085a1fdae525    7.9 GB    27 hours ago    \n",
      "llama3.2:3b               a80c4f17acd5    2.0 GB    32 hours ago    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([\"ollama\", \"list\"], text=True, capture_output=True)\n",
    "    print(result.stdout)  # Prints the output of the command\n",
    "except Exception as e:\n",
    "    print(f\"Error executing command: {e}\")\n",
    "\n",
    "# export to list\n",
    "output=result.stdout.split('\\n')\n",
    "\n",
    "models = []\n",
    "\n",
    "for model in output[1:-1]:\n",
    "    #remove 8 characters from the beginning of the string\n",
    "    model=model[:-42].strip()\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llava:7b', 'llama3.2-vision:latest', 'llama3.2:3b']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\izardy\\AppData\\Local\\Temp\\ipykernel_14736\\445291172.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model='gpt-4-turbo')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model='gpt-4-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-turbo', 'system_fingerprint': 'fp_f17929ee92', 'finish_reason': 'stop', 'logprobs': None}, id='run-ae0de308-95e6-470a-adb3-52d4e9bdf708-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the capital of France?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
