{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def describe_image(image_path):\n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Validate API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OpenAI API key not found in environment variables\")\n",
    "            \n",
    "        # Validate image path\n",
    "        if not image_path:\n",
    "            raise ValueError(\"Image path cannot be empty\")\n",
    "\n",
    "        # Convert the image to base64\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Using the correct model for vision\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error describing image: {str(e)}\")\n",
    "    \n",
    "def extract_text_from_img(image_path):\n",
    "    try:\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        \n",
    "        # Validate API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OpenAI API key not found in environment variables\")\n",
    "            \n",
    "        # Validate image path\n",
    "        if not image_path:\n",
    "            raise ValueError(\"Image path cannot be empty\")\n",
    "\n",
    "        # Convert the image to base64\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",  # Using the correct model for vision\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Extract text from this image only. Do not include any other information.\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error describing image: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_image(image_path='img.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_from_img(image_path='img.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.llms import Ollama\n",
    "from PIL import Image\n",
    "import PyPDF2\n",
    "import base64\n",
    "import io\n",
    "\n",
    "def init_ollama():\n",
    "    return Ollama(model=\"llama3.2:latest\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def describe_image(image_file):\n",
    "    try:\n",
    "        # Open and process the image\n",
    "        image = Image.open(image_file)\n",
    "        \n",
    "        # Convert PIL Image to base64 string\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "        \n",
    "        # Create a more detailed prompt\n",
    "        prompt = (\"Provide a detailed description of this image, including: \"\n",
    "                 \"main subjects, colors, composition, lighting, setting, \"\n",
    "                 \"and any notable details or activities shown.\")\n",
    "    \n",
    "        try:\n",
    "            # Initialize Ollama with vision model\n",
    "            llm = init_ollama()\n",
    "            \n",
    "            # Pass the base64 encoded image string\n",
    "            description = llm(prompt, images=[img_str])\n",
    "            return description\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error generating description: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading image: {str(e)}\")\n",
    "\n",
    "# Create the Streamlit interface\n",
    "st.title('Sandbox for Marketing Materials Compliance Analysis')\n",
    "\n",
    "# Initialize chat history in session state if it doesn't exist\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.file_uploader(\"Upload a PDF or Image file\", type=['pdf', 'png', 'jpg', 'jpeg'])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    try:\n",
    "        # Process based on file type\n",
    "        file_type = uploaded_file.type\n",
    "        if 'pdf' in file_type:\n",
    "            text_content = extract_text_from_pdf(uploaded_file)\n",
    "            st.success(\"PDF processed successfully!\")\n",
    "            \n",
    "            # Create prompt for summarization\n",
    "            prompt = f\"Please summarize the following text:\\n\\n{text_content}\"\n",
    "            \n",
    "            # Generate summary using Ollama\n",
    "            llm = init_ollama()\n",
    "            response = llm(prompt)\n",
    "            \n",
    "        elif 'image' in file_type:\n",
    "            # Display the uploaded image\n",
    "            st.image(uploaded_file, caption=\"Uploaded Image\")\n",
    "            \n",
    "            # Get image description\n",
    "            response = describe_image(uploaded_file)\n",
    "            st.success(\"Image processed successfully!\")\n",
    "        \n",
    "        # Add the response to chat history\n",
    "        st.session_state.messages.append({\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": f\"Analysis of uploaded file:\\n\\n{response}\"\n",
    "        })\n",
    "\n",
    "        st.session_state.uploaded_file = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Chat input for follow-up questions\n",
    "if prompt := st.chat_input(\"Ask questions about the uploaded content\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Display user message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Generate response\n",
    "    try:\n",
    "        llm = init_ollama()\n",
    "        response = llm(prompt)\n",
    "\n",
    "        # Display assistant response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(response)\n",
    "        \n",
    "        # Add assistant response to chat history\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error communicating with Ollama: {str(e)}\")\n",
    "\n",
    "# Add a button to clear chat history\n",
    "if st.button(\"Clear Chat History\"):\n",
    "    st.session_state.messages = []\n",
    "    st.experimental_rerun()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
